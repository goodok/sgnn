# @package _group_

#  https://pytorch-lightning.readthedocs.io/en/latest/trainer.html# trainer-class
# Remarks:
#  - all None fields are commented, as hydra treats them as string.

gpus: 1
distributed_backend: dp
precision: 32

logger: True
checkpoint_callback: True
early_stop_callback: False
# callbacks: None
# default_root_dir: None
gradient_clip_val: 0
# process_position: 0
# num_nodes: 1
# num_processes: 1
# gpus: None
# auto_select_gpus: False
# tpu_cores: None
# log_gpu_memory: None
# progress_bar_refresh_rate: 1
overfit_batches: 0.0
# track_grad_norm: -1
# check_val_every_n_epoch: 1
# fast_dev_run: False
accumulate_grad_batches: 1
# #  max_epochs: 1000  #  use train.max_epochs
# min_epochs: 1
# max_steps: None
# min_steps: None
limit_train_batches: 1.0
limit_val_batches: 1.0
limit_test_batches: 1.0
val_check_interval: 1.0
log_save_interval: 100
row_log_interval: 50
# distributed_backend: None
# precision: 32
weights_summary: 'top'
# weights_save_path: None
num_sanity_val_steps: 2
# truncated_bptt_steps: None
# resume_from_checkpoint: None
# profiler: None
benchmark: False
deterministic: False
reload_dataloaders_every_epoch: False
auto_lr_find: False
# replace_sampler_ddp: True
terminate_on_nan: False
auto_scale_batch_size: False
prepare_data_per_node: True
amp_level: 'O2'
# val_percent_check: None    Depricated
# test_percent_check: None
# train_percent_check: None
# overfit_pct: None
